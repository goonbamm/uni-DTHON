{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_from_csv(target):\n",
    "      return pd.read_csv(target).rename(columns=lambda x: x.strip())\n",
    "\n",
    "def dataframe_from_csvs(targets):\n",
    "  return pd.concat([dataframe_from_csv(x) for x in targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 62564\n",
      "validation: 7820\n",
      "test: 7820\n"
     ]
    }
   ],
   "source": [
    "data_path = 'dataset'\n",
    "\n",
    "train_files = sorted([x for x in Path(f'{data_path}/train/').glob('*.csv')])\n",
    "val_files = sorted([x for x in Path(f'{data_path}/val/').glob('*.csv')])\n",
    "\n",
    "train = dataframe_from_csvs(train_files)\n",
    "val = dataframe_from_csvs(val_files)\n",
    "test = pd.read_csv(f'{data_path}/test.csv')\n",
    "print(f'train: {len(train)}')\n",
    "print(f'validation: {len(val)}')\n",
    "print(f'test: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['leaktype'].replace(['out','in','noise','other','normal'], [0,1,2,3,4], inplace=True)\n",
    "val['leaktype'].replace(['out','in','noise','other','normal'], [0,1,2,3,4], inplace=True)\n",
    "test['leaktype']=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    \"\"\"\n",
    "    column 별로 정규화를 시키는 함수입니다.\n",
    "    이때, 정규화 방식은 표준편차가 아닌 최대 - 최소로 구하였습니다.\n",
    "    진행한 이유는 특정 column 값이 큰 영향을 끼칠 수 없도록 제한하기 위함입니다.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): csv 파일을 pandas 로 읽은 데이터\n",
    "\n",
    "    Returns:\n",
    "        result (DataFrame) : df 를 column 별로 정규화한 데이터\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    for feature_name in df.columns:\n",
    "        # site, sid, leaktype 은 정규화 대상이 아니므로, 값을 그대로 저장하고 넘어갑니다.\n",
    "        if feature_name in ['site', 'sid', 'leaktype']:\n",
    "            result[feature_name] = df[feature_name]\n",
    "            continue\n",
    "    \n",
    "        # C01 ~ C26\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value) * 100\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2alpha(char_dict):\n",
    "    \"\"\"\n",
    "    숫자를 자연어(영어 형태)로 변환하는 함수입니다.\n",
    "    \n",
    "    영어를 택한 이유는 크게 2가지입니다.\n",
    "    - C01 ~ C26 이 26개로 알파벳 개수와 동일하다.\n",
    "    - 현재 NLP 에서 가장 뛰어난 모델은 대부분 영어 데이터에 최적화되어 있다.\n",
    "    \n",
    "    결론부터 말씀드리면 결과는 다음 예시처럼 나옵니다.\n",
    "    \n",
    "    예시: 'site: S-4784025026. sid: S-0359369085186035. aa.... zz.'\n",
    "    \n",
    "    변환하는 방식은 다음과 같습니다.\n",
    "    \n",
    "    1. 각 row 별로 백분율을 구합니다.\n",
    "    2. 그리고 백분율에 400 을 곱합니다.\n",
    "    여기서 400 을 곱하는 이유는 'BERT'의 최대 입력 길이를 고려했기 때문입니다.\n",
    "    최대 길이가 512 인데, site 와 sid 를 제일 앞에 표기한 길이를 대강 112 로 정했습니다.\n",
    "    그리고 그 후, 나머지 길이 400 을 나머지 row 끼리 나눠서 알파벳을 나열하는 구조입니다.\n",
    "    3. 이때, 알파벳별로 간격을 두었습니다.\n",
    "\n",
    "    Args:\n",
    "        char_dict (Dict): C01 ~ C26 값을 모두 저장한 딕셔너리 자료형\n",
    "\n",
    "    Returns:\n",
    "        str : 자연어로 변환된 문자열\n",
    "    \"\"\"\n",
    "    sentence = list()\n",
    "    \n",
    "    s = sum(char_dict.values())\n",
    "    for k, v in char_dict.items():\n",
    "        a = round(v / s * 400) * chr(96 + int(k[1:]))\n",
    "        sentence.append(a)\n",
    "\n",
    "    return ' '.join(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablet2nlp(df):\n",
    "    \"\"\"\n",
    "    : tablet 데이터를 자연어 데이터로 변환하는 함수\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): csv 파일을 읽은 데이터\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: 자연어로 변환된 데이터\n",
    "    \"\"\"\n",
    "    nlp_list = list()\n",
    "    \n",
    "    for i, v in df.iterrows():\n",
    "        nlp_data = list() # 최종적으로 자연어를 저장하는 변수\n",
    "        char_dict = dict()  # 'C01' ~ 'C26' 을 저장하는 변수\n",
    "\n",
    "        for c in df.columns:\n",
    "            if c.startswith('C'):\n",
    "                char_dict[c] = v[c]\n",
    "                \n",
    "            elif c.startswith('si'): # site, sid\n",
    "                nlp_data.append(f'{c}: {v[c]}.')\n",
    "        \n",
    "        nlp_data.append(num2alpha(char_dict))\n",
    "        nlp_list.append(' '.join(nlp_data) + '.')\n",
    "    \n",
    "    df['nlp'] = nlp_list\n",
    "\n",
    "    # label 과 자연어 데이터 이외에는 모두 삭제합니다.\n",
    "    df = df[['leaktype', 'nlp']]\n",
    "\n",
    "    return df                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2nlp(df):\n",
    "    \"\"\"\n",
    "    실질적으로 부르는 함수.\n",
    "    1. 정규화하고,\n",
    "    2. 자연어로 변환한다.\n",
    "    \n",
    "    간단히 표현하고자 있는 함수입니다.\n",
    "    \"\"\"\n",
    "    df = normalize(df)\n",
    "    return tablet2nlp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 모두 자연어 형태로 변환한다.\n",
    "nlp_train = data2nlp(train)\n",
    "nlp_val = data2nlp(val)\n",
    "nlp_test = data2nlp(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 모두 저정해둡니다. 이때, index=False\n",
    "nlp_train.to_csv('dataset/nlp/train.csv', index=False)\n",
    "nlp_val.to_csv('dataset/nlp/val.csv', index=False)\n",
    "nlp_test.to_csv('dataset/nlp/test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d863ddbeeb12e59c8b47eb7d0544679a05a22030947c1d38f26e21aa2796d5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
