{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **구글 드라이브 마운트**"
      ],
      "metadata": {
        "id": "U7GwJ5N8Cs7J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3qIBnA9pdH1",
        "outputId": "f05361d4-c2be-4143-fffb-ca483fac324b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **외부 라이브러리 설치**"
      ],
      "metadata": {
        "id": "Jx7BCnaxC4Xi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBbKg37ypoyu"
      },
      "outputs": [],
      "source": [
        "# transformer: NLP model\n",
        "# evaluate: metric for F1 score\n",
        "!pip install transformers evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Libraries**"
      ],
      "metadata": {
        "id": "63jxCEgpC97N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2v4JSJ8pq7t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import shutil\n",
        "import evaluate\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from torch.optim import AdamW\n",
        "from pytorch_optimizer import Ranger21\n",
        "from torch.cuda.amp import autocast\n",
        "from torch.cuda.amp import GradScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, get_scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GPU Setting**"
      ],
      "metadata": {
        "id": "LBx_tYKHDC2k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkW0l7GRpzEf",
        "outputId": "6ab5fcd1-c691-4c5a-8ca8-b3f757b77a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:cuda\n"
          ]
        }
      ],
      "source": [
        "# GPU 사용을 위해 cuda 사용이 가능한지 확인합니다.\n",
        "# 'CPU'라고 나타나는 경우, colab 런타임 유형 변경을 통해 바꾸셔야 합니다. \n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'device:{device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hyperparameter Setting**"
      ],
      "metadata": {
        "id": "b8kiBpFgDJdY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56-dGfGrp0vA"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "checkpoint = 'bert-base-uncased'\n",
        "num_labels = 5\n",
        "model_path = f'model_with_{checkpoint}'\n",
        "\n",
        "# training\n",
        "use_amp = True\n",
        "num_epochs = 100\n",
        "lr = 1e-5 # 학습 양상이 처음부터 계속 valid loss 가 증가하는 방향이라 2e-5 에서 낮추게 되었습니다.\n",
        "batch_size = 32\n",
        "\n",
        "# scheduler\n",
        "scheduler_name = 'linear'\n",
        "num_warmup_steps = 0\n",
        "\n",
        "# early_stop\n",
        "early_stop = 3 # 좋아지는 경우가 거의 없어서 GPU 사용량을 효율적으로 운영하고자 값을 줄였습니다.\n",
        "\n",
        "# seed\n",
        "seed = 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Seed Setting**\n",
        "\n",
        "- 최대한 REPRODUCIBILITY 를 보장하기 위해 제어합니다.\n",
        "- 단, pytorch 역시 공식적으로 완전히 보장할 수 없다고 말합니다.\n",
        "\n",
        "- pytorch link: https://pytorch.org/docs/stable/notes/randomness.html"
      ],
      "metadata": {
        "id": "hlyWyZ6_DNK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhCSQxOdrcJo"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int = 42, contain_cuda: bool = False):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    print(f\"Seed set as {seed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiqcjbM5re4g",
        "outputId": "4c0d7da2-8b05-4cd2-c855-c8b961262d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set as 2022\n"
          ]
        }
      ],
      "source": [
        "seed_everything(seed=2022)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Original CSV File**"
      ],
      "metadata": {
        "id": "jnFivwFDFO0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_from_csv(target):\n",
        "      return pd.read_csv(target).rename(columns=lambda x: x.strip())\n",
        "\n",
        "def dataframe_from_csvs(targets):\n",
        "  return pd.concat([dataframe_from_csv(x) for x in targets])"
      ],
      "metadata": {
        "id": "uvlnKXKdEhrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/dataset'\n",
        "\n",
        "train_files = sorted([x for x in Path(f'{data_path}/train/').glob('*.csv')])\n",
        "val_files = sorted([x for x in Path(f'{data_path}/val/').glob('*.csv')])\n",
        "\n",
        "train = dataframe_from_csvs(train_files)\n",
        "val = dataframe_from_csvs(val_files)\n",
        "test = pd.read_csv(f'{data_path}/test.csv')\n",
        "print(f'train: {len(train)}')\n",
        "print(f'validation: {len(val)}')\n",
        "print(f'test: {len(test)}')"
      ],
      "metadata": {
        "id": "KyQtLrSjEjO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['leaktype'].replace(['out','in','noise','other','normal'], [0,1,2,3,4], inplace=True)\n",
        "val['leaktype'].replace(['out','in','noise','other','normal'], [0,1,2,3,4], inplace=True)\n",
        "test['leaktype']=\"\""
      ],
      "metadata": {
        "id": "jqoCzu-wEj5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Normalize Dataset**"
      ],
      "metadata": {
        "id": "JMLSpSAsFUYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(df):\n",
        "    \"\"\"\n",
        "    column 별로 정규화를 시키는 함수입니다.\n",
        "    이때, 정규화 방식은 표준편차가 아닌 최대 - 최소로 구하였습니다.\n",
        "    진행한 이유는 특정 column 값이 큰 영향을 끼칠 수 없도록 제한하기 위함입니다.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): csv 파일을 pandas 로 읽은 데이터\n",
        "\n",
        "    Returns:\n",
        "        result (DataFrame) : df 를 column 별로 정규화한 데이터\n",
        "    \"\"\"\n",
        "    result = df.copy()\n",
        "    \n",
        "    for feature_name in df.columns:\n",
        "        # site, sid, leaktype 은 정규화 대상이 아니므로, 값을 그대로 저장하고 넘어갑니다.\n",
        "        if feature_name in ['site', 'sid', 'leaktype']:\n",
        "            result[feature_name] = df[feature_name]\n",
        "            continue\n",
        "    \n",
        "        # C01 ~ C26\n",
        "        max_value = df[feature_name].max()\n",
        "        min_value = df[feature_name].min()\n",
        "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value) * 100\n",
        "        \n",
        "    return result"
      ],
      "metadata": {
        "id": "k6WdA1PvElUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num2alpha(char_dict):\n",
        "    \"\"\"\n",
        "    숫자를 자연어(영어 형태)로 변환하는 함수입니다.\n",
        "    \n",
        "    영어를 택한 이유는 크게 2가지입니다.\n",
        "    - C01 ~ C26 이 26개로 알파벳 개수와 동일하다.\n",
        "    - 현재 NLP 에서 가장 뛰어난 모델은 대부분 영어 데이터에 최적화되어 있다.\n",
        "    \n",
        "    결론부터 말씀드리면 결과는 다음 예시처럼 나옵니다.\n",
        "    \n",
        "    예시: 'site: S-4784025026. sid: S-0359369085186035. aa.... zz.'\n",
        "    \n",
        "    변환하는 방식은 다음과 같습니다.\n",
        "    \n",
        "    1. 각 row 별로 백분율을 구합니다.\n",
        "    2. 그리고 백분율에 400 을 곱합니다.\n",
        "    여기서 400 을 곱하는 이유는 'BERT'의 최대 입력 길이를 고려했기 때문입니다.\n",
        "    최대 길이가 512 인데, site 와 sid 를 제일 앞에 표기한 길이를 대강 112 로 정했습니다.\n",
        "    그리고 그 후, 나머지 길이 400 을 나머지 row 끼리 나눠서 알파벳을 나열하는 구조입니다.\n",
        "    3. 이때, 알파벳별로 간격을 두었습니다.\n",
        "\n",
        "    Args:\n",
        "        char_dict (Dict): C01 ~ C26 값을 모두 저장한 딕셔너리 자료형\n",
        "\n",
        "    Returns:\n",
        "        str : 자연어로 변환된 문자열\n",
        "    \"\"\"\n",
        "    sentence = list()\n",
        "    \n",
        "    s = sum(char_dict.values())\n",
        "    for k, v in char_dict.items():\n",
        "        a = round(v / s * 400) * chr(96 + int(k[1:]))\n",
        "        sentence.append(a)\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "    "
      ],
      "metadata": {
        "id": "6q67TrgrEmuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num2alpha(char_dict):\n",
        "    \"\"\"\n",
        "    숫자를 자연어(영어 형태)로 변환하는 함수입니다.\n",
        "    \n",
        "    영어를 택한 이유는 크게 2가지입니다.\n",
        "    - C01 ~ C26 이 26개로 알파벳 개수와 동일하다.\n",
        "    - 현재 NLP 에서 가장 뛰어난 모델은 대부분 영어 데이터에 최적화되어 있다.\n",
        "    \n",
        "    결론부터 말씀드리면 결과는 다음 예시처럼 나옵니다.\n",
        "    \n",
        "    예시: 'site: S-4784025026. sid: S-0359369085186035. aa.... zz.'\n",
        "    \n",
        "    변환하는 방식은 다음과 같습니다.\n",
        "    \n",
        "    1. 각 row 별로 백분율을 구합니다.\n",
        "    2. 그리고 백분율에 400 을 곱합니다.\n",
        "    여기서 400 을 곱하는 이유는 'BERT'의 최대 입력 길이를 고려했기 때문입니다.\n",
        "    최대 길이가 512 인데, site 와 sid 를 제일 앞에 표기한 길이를 대강 112 로 정했습니다.\n",
        "    그리고 그 후, 나머지 길이 400 을 나머지 row 끼리 나눠서 알파벳을 나열하는 구조입니다.\n",
        "    3. 이때, 알파벳별로 간격을 두었습니다.\n",
        "\n",
        "    Args:\n",
        "        char_dict (Dict): C01 ~ C26 값을 모두 저장한 딕셔너리 자료형\n",
        "\n",
        "    Returns:\n",
        "        str : 자연어로 변환된 문자열\n",
        "    \"\"\"\n",
        "    sentence = list()\n",
        "    \n",
        "    s = sum(char_dict.values())\n",
        "    for k, v in char_dict.items():\n",
        "        a = round(v / s * 400) * chr(96 + int(k[1:]))\n",
        "        sentence.append(a)\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "    "
      ],
      "metadata": {
        "id": "99MrAcp_Eo6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tabular Dataset to Natural Language Dataset**"
      ],
      "metadata": {
        "id": "1eTmMLqSFdkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data2nlp(df):\n",
        "    \"\"\"\n",
        "    실질적으로 부르는 함수.\n",
        "    1. 정규화하고,\n",
        "    2. 자연어로 변환한다.\n",
        "    \n",
        "    간단히 표현하고자 있는 함수입니다.\n",
        "    \"\"\"\n",
        "    df = normalize(df)\n",
        "    return tablet2nlp(df)"
      ],
      "metadata": {
        "id": "tgoeUEqeEr_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 모두 자연어 형태로 변환한다.\n",
        "nlp_train = data2nlp(train)\n",
        "nlp_val = data2nlp(val)\n",
        "nlp_test = data2nlp(test)"
      ],
      "metadata": {
        "id": "BGXpBFaaEtld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 모두 저정해둡니다. 이때, index=False\n",
        "nlp_train.to_csv('dataset/nlp/train.csv', index=False)\n",
        "nlp_val.to_csv('dataset/nlp/val.csv', index=False)\n",
        "nlp_test.to_csv('dataset/nlp/test.csv', index=False)"
      ],
      "metadata": {
        "id": "-endoKt8EuC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load CSV Natural Language file**"
      ],
      "metadata": {
        "id": "mNyIjViXDqkh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIkHUILPrfOD"
      },
      "outputs": [],
      "source": [
        "data_path = 'dataset/nlp'\n",
        "\n",
        "train_csv = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
        "valid_csv = pd.read_csv(os.path.join(data_path, 'val.csv'))\n",
        "test_csv = pd.read_csv(os.path.join(data_path, 'test.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Customize Dataset**"
      ],
      "metadata": {
        "id": "NkK-Kmt7Du1D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iAVnOPisodz"
      },
      "outputs": [],
      "source": [
        "class UnidthonDataset(Dataset):\n",
        "  def __init__(self, csv, tokenizer):\n",
        "    self.dataset = tokenizer(list(csv['nlp']), truncation=True, padding=True, return_tensors='pt')\n",
        "    self.labels = list(csv['leaktype'])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    item = {k: torch.tensor(v[idx]) for k, v in self.dataset.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[idx])\n",
        "\n",
        "    item = {k: torch.tensor(v[idx]) for k, v in self.dataset.items()}\n",
        "    if np.isnan(self.labels[idx]): # test dataset 은 값이 비어 있어, NAN 을 방지하기 위해 '0'을 기입합니다.\n",
        "        item['labels'] = 0\n",
        "        \n",
        "    else: \n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "\n",
        "    return item"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Tokenizer**"
      ],
      "metadata": {
        "id": "q017i3q9D8mN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiqxLZ0EtmA-"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CSV to DataLoader**"
      ],
      "metadata": {
        "id": "UR6AmAT-EEZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpINLhFtuOlL"
      },
      "outputs": [],
      "source": [
        "def csv_to_dataloader(csv, tokenizer, mode, batch_size):\n",
        "  shuffle = True if mode in ['train', 'valid'] else False\n",
        "  dataset = UnidthonDataset(csv, tokenizer)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "  return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH_WKpcX-p3g"
      },
      "outputs": [],
      "source": [
        "# dataset\n",
        "train_dataset = UnidthonDataset(train_csv, tokenizer)\n",
        "valid_dataset = UnidthonDataset(valid_csv, tokenizer)\n",
        "test_dataset = UnidthonDataset(test_csv, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4aJwQLDtrPR"
      },
      "outputs": [],
      "source": [
        "# dataloader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # test should not be shuffled !"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Model**"
      ],
      "metadata": {
        "id": "1rz6FW_UELjf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xviUYztCxpNt"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
        "                                                            num_labels=num_labels)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Optimizer & Scheduler**"
      ],
      "metadata": {
        "id": "8CvW-6DHEPFx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a24SWAGGxqUf"
      },
      "outputs": [],
      "source": [
        "# optimizer & scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(name=scheduler_name, optimizer=optimizer,\n",
        "                              num_warmup_steps=num_warmup_steps,\n",
        "                              num_training_steps=num_training_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Make Method to infer per epoch**\n",
        "\n",
        "- 코드의 간결화를 위해 train, eval 모두 하나의 함수에서 가능하도록 만들었습니다."
      ],
      "metadata": {
        "id": "lXAvRa6LEW_c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4iklSeUwCDf"
      },
      "outputs": [],
      "source": [
        "def inference_per_epoch(tokenizer, model, optimizer, scheduler, use_amp, scaler, dataloader, mode):\n",
        "  if mode == 'train':\n",
        "    model.train()\n",
        "  \n",
        "  else: # 'valid' or 'test'\n",
        "    model.eval()\n",
        "\n",
        "  total_loss = 0\n",
        "  metric = evaluate.load('f1')\n",
        "\n",
        "  for batch in tqdm(dataloader, desc=f'{mode} per epoch'):\n",
        "    batch = { k: v.to(device) for k, v in batch.items() }\n",
        "\n",
        "    if mode == 'train':\n",
        "      # colab 인 점을 고려하여, GPU 사용량 및 시간을 절약하고자 사용합니다.\n",
        "      with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "      \n",
        "      scaler.scale(loss).backward()\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "      loss = outputs.loss\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch['labels'])\n",
        "\n",
        "  total_loss /= len(dataloader)\n",
        "  f1 = metric.compute(average='micro')['f1']\n",
        "\n",
        "  return total_loss, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Loops**"
      ],
      "metadata": {
        "id": "6lo5cpPoE3av"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxruerUTvTo4"
      },
      "outputs": [],
      "source": [
        "best_model_path = 'best_model'\n",
        "best_performance = 0\n",
        "best_epoch = 0\n",
        "patience = 0 # for Early Stop\n",
        "\n",
        "scaler = GradScaler(enabled=use_amp)\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), desc='Training loops'):\n",
        "  # train per epoch\n",
        "  train_loss, train_f1 = inference_per_epoch(tokenizer=tokenizer, model=model,\n",
        "                                              optimizer=optimizer, scheduler=lr_scheduler,\n",
        "                                              use_amp=use_amp, scaler=scaler,\n",
        "                                              dataloader=train_dataloader, mode='train')\n",
        "\n",
        "  # evaluate per epoch\n",
        "  valid_loss, valid_f1 = inference_per_epoch(tokenizer=tokenizer, model=model,\n",
        "                                              optimizer=optimizer, scheduler=lr_scheduler,\n",
        "                                              use_amp=use_amp, scaler=scaler,\n",
        "                                              dataloader=valid_dataloader, mode='valid')\n",
        "\n",
        "  print(f'epoch: {epoch}')\n",
        "  print(f'train_loss: {train_loss}, train_f1: {train_f1}')\n",
        "  print(f'valid_loss: {valid_loss}, valid_f1: {valid_f1}\\n')\n",
        "\n",
        "  if best_performance < valid_f1:\n",
        "    patience = 0\n",
        "\n",
        "    print(f'best performance: {best_performance} → {valid_f1}\\n')\n",
        "\n",
        "    # save best model\n",
        "    if os.path.exists(best_model_path):\n",
        "      shutil.rmtree(best_model_path) # delete everything in the directory\n",
        "\n",
        "    model.save_pretrained(best_model_path)\n",
        "\n",
        "    best_performance = valid_f1\n",
        "    best_epoch = epoch\n",
        "\n",
        "    print(f'saving best model at epoch: {epoch}')\n",
        "    print(f'best performance: {best_performance} → {valid_f1}\\n')\n",
        "\n",
        "  else:\n",
        "    patience += 1\n",
        "\n",
        "  # early_stop\n",
        "  if early_stop <= patience:\n",
        "    print('----- Early Stop -----')\n",
        "    print(f'→ number of patience: {patience}\\n')\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test the model**"
      ],
      "metadata": {
        "id": "Fyz-JFBZFkhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load best model\n",
        "best_model = AutoModelForSequenceClassification.from_pretrained(best_model_path, num_labels=num_labels)\n",
        "best_model.to(device)\n",
        "best_model.eval()\n",
        "\n",
        "# inference\n",
        "answer_list = list()\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = { k: v.to(device) for k, v in batch.items() }\n",
        "    with torch.no_grad():\n",
        "        outputs = best_model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    answer_list += predictions.detach().cpu().tolist()"
      ],
      "metadata": {
        "id": "_Urj0u-TRnge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Save the result**"
      ],
      "metadata": {
        "id": "du3HbXxRF8vU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_csv = pd.DataFrame()\n",
        "result_csv['site'] = test['site']\n",
        "result_csv['sid'] = test['sid']\n",
        "result_csv['leaktype'] = answer_list\n",
        "\n",
        "result_csv.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "Mz9PIPvvS270"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_csv.head()"
      ],
      "metadata": {
        "id": "XHNvOBCoS5Aq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}